+ export CUDA_VISIBLE_DEVICES=0,1,2,3
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ lsof -i :8000
+ '[' '!' -f /p/project1/hai_kg-rag-thesis/scripts/Apptainer_parallel/logs/10-02-2025/deepseek_job_10965734/misc/gpu_usage.csv ']'
+ echo 'timestamp,index,name,utilization.gpu [%],memory.used [MiB],memory.total [MiB]'
+ GPU_MONITOR_PID=5628
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ echo 'Starting file tracking...'
+ tee -a /p/project1/hai_kg-rag-thesis/scripts/Apptainer_parallel/logs/10-02-2025/deepseek_job_10965734/track_files.log
+ python /p/project1/hai_kg-rag-thesis/scripts/Apptainer_parallel/track_files.py --root-dir /p/project1/hai_kg-rag-thesis/scripts/Apptainer_parallel --output /p/project1/hai_kg-rag-thesis/scripts/Apptainer_parallel/logs/10-02-2025/deepseek_job_10965734/misc/meta
+ sleep 5
+ '[' 0 -ne 0 ']'
+ echo 'File tracking completed successfully!'
+ tee -a /p/project1/hai_kg-rag-thesis/scripts/Apptainer_parallel/logs/10-02-2025/deepseek_job_10965734/track_files.log
+ apptainer exec --nv --pwd /app --bind /p/project1/hai_kg-rag-thesis/models:/models /p/project1/hai_kg-rag-thesis/llama_server.sif ./llama-server -m /p/project1/hai_kg-rag-thesis/models/unsloth/DeepSeek-V3-GGUF/DeepSeek-V3-Q2_K_XS/DeepSeek-V3-Q2_K_XS-00001-of-00005.gguf --port 8000 --host 0.0.0.0 -n 512 -c 4096 --n-gpu-layers 35 --parallel 4 --cache-type-k q5_0 --mlock --rope-freq-base 1000000
+ SERVER_PID=5639
+ echo 'Server PID: 5639'
+ sleep 100
+ tee -a /p/project1/hai_kg-rag-thesis/scripts/Apptainer_parallel/logs/10-02-2025/deepseek_job_10965734/server.log
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ echo 'Waiting for LLM server to start on port 8000...'
++ seq 1
+ for i in $(seq 1 $LLM_SERVER_RETRIES)
+ nc -z 0.0.0.0 8000
+ echo 'LLM server is ready!'
+ break
+ nc -z 0.0.0.0 8000
+ echo 'Calling LLM API...'
+ tee -a /p/project1/hai_kg-rag-thesis/scripts/Apptainer_parallel/logs/10-02-2025/deepseek_job_10965734/call_llm_api.log
+ python /p/project1/hai_kg-rag-thesis/scripts/Apptainer_parallel/call_llm_api.py --host 0.0.0.0 --port 8000 --model_path /p/project1/hai_kg-rag-thesis/models/unsloth/DeepSeek-V3-GGUF/DeepSeek-V3-Q2_K_XS/DeepSeek-V3-Q2_K_XS-00001-of-00005.gguf --prompt_path /p/project1/hai_kg-rag-thesis/SPARQL_test/LLM_Prompts/Test_V1.txt --output_dir /p/project1/hai_kg-rag-thesis/scripts/Apptainer_parallel/logs/10-02-2025/deepseek_job_10965734/misc/temp --max_tokens 256 --temperature 0.3 --model_name DeepSeek-V3-Q2_K_XS
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ '[' 0 -ne 0 ']'
+ echo 'LLM API call completed successfully!'
+ tee -a /p/project1/hai_kg-rag-thesis/scripts/Apptainer_parallel/logs/10-02-2025/deepseek_job_10965734/call_llm_api.log
+ kill 5628
+ sleep 5
+ echo 'Generating GPU usage graph...'
+ tee -a /p/project1/hai_kg-rag-thesis/scripts/Apptainer_parallel/logs/10-02-2025/deepseek_job_10965734/gpu_graph.log
+ python /p/project1/hai_kg-rag-thesis/scripts/Apptainer_parallel/gpu_graph.py --csv_file /p/project1/hai_kg-rag-thesis/scripts/Apptainer_parallel/logs/10-02-2025/deepseek_job_10965734/misc/gpu_usage.csv --output_image /p/project1/hai_kg-rag-thesis/scripts/Apptainer_parallel/logs/10-02-2025/deepseek_job_10965734/misc/gpu_usage.png
+ '[' 1 -ne 0 ']'
+ echo 'ERROR: GPU visualization failed. Check logs: /p/project1/hai_kg-rag-thesis/scripts/Apptainer_parallel/logs/10-02-2025/deepseek_job_10965734/gpu_graph.err'
+ tee -a /p/project1/hai_kg-rag-thesis/scripts/Apptainer_parallel/logs/10-02-2025/deepseek_job_10965734/gpu_graph.log
+ exit 1
