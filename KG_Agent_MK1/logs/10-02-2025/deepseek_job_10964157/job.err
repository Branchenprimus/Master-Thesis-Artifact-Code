+ export CUDA_VISIBLE_DEVICES=0,1,2,3
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ lsof -i :8000
+ '[' '!' -f /p/project1/hai_kg-rag-thesis/scripts/Apptainer_parallel/logs/10-02-2025/deepseek_job_10964157/misc/gpu_usage.csv ']'
+ echo 'timestamp,index,name,utilization.gpu [%],memory.used [MiB],memory.total [MiB]'
+ GPU_MONITOR_PID=3682
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ echo 'Starting file tracking...'
+ tee -a /p/project1/hai_kg-rag-thesis/scripts/Apptainer_parallel/logs/10-02-2025/deepseek_job_10964157/track_files.log
+ python3 /p/project1/hai_kg-rag-thesis/scripts/Apptainer_parallel/track_files.py --root-dir /p/project1/hai_kg-rag-thesis/scripts/Apptainer_parallel --output /p/project1/hai_kg-rag-thesis/scripts/Apptainer_parallel/logs/10-02-2025/deepseek_job_10964157/misc/meta
+ sleep 5
+ '[' 0 -ne 0 ']'
+ echo 'File tracking completed successfully!'
+ tee -a /p/project1/hai_kg-rag-thesis/scripts/Apptainer_parallel/logs/10-02-2025/deepseek_job_10964157/track_files.log
+ apptainer exec --nv --pwd /app --bind /p/project1/hai_kg-rag-thesis/models:/models /p/project1/hai_kg-rag-thesis/llama_server.sif ./llama-server -m /p/project1/hai_kg-rag-thesis/models/unsloth/DeepSeek-V3-GGUF/DeepSeek-V3-Q2_K_XS/DeepSeek-V3-Q2_K_XS-00001-of-00005.gguf --port 8000 --host 0.0.0.0 -n 512 -c 4096 --n-gpu-layers 35 --parallel 4 --cache-type-k q5_0 --mlock --rope-freq-base 1000000
+ SERVER_PID=3693
+ echo 'Server PID: 3693'
+ sleep 100
+ tee -a /p/project1/hai_kg-rag-thesis/scripts/Apptainer_parallel/logs/10-02-2025/deepseek_job_10964157/server.log
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ true
+ nvidia-smi --query-gpu=timestamp,index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
+ sleep 5
+ echo 'Waiting for LLM server to start on port 8000...'
++ seq 1
+ for i in $(seq 1 $LLM_SERVER_RETRIES)
+ nc -z 0.0.0.0 8000
+ echo 'LLM server is ready!'
+ break
+ nc -z 0.0.0.0 8000
+ echo 'Calling LLM API...'
+ tee -a /p/project1/hai_kg-rag-thesis/scripts/Apptainer_parallel/logs/10-02-2025/deepseek_job_10964157/call_llm_api.log
+ python3 /p/project1/hai_kg-rag-thesis/scripts/call_llm_api.py --host 0.0.0.0 --port 8000 --model_path /p/project1/hai_kg-rag-thesis/models/unsloth/DeepSeek-V3-GGUF/DeepSeek-V3-Q2_K_XS/DeepSeek-V3-Q2_K_XS-00001-of-00005.gguf --prompt_path /p/project1/hai_kg-rag-thesis/SPARQL_test/LLM_Prompts/Test_V1.txt --output_dir /p/project1/hai_kg-rag-thesis/scripts/Apptainer_parallel/logs/10-02-2025/deepseek_job_10964157/misc/temp --max_tokens 256 --temperature 0.3 --model_name DeepSeek-V3-Q2_K_XS
+ '[' 2 -ne 0 ']'
+ echo 'ERROR: LLM API call failed. Check logs: /p/project1/hai_kg-rag-thesis/scripts/Apptainer_parallel/logs/10-02-2025/deepseek_job_10964157/call_llm_api.err'
+ tee -a /p/project1/hai_kg-rag-thesis/scripts/Apptainer_parallel/logs/10-02-2025/deepseek_job_10964157/call_llm_api.log
+ exit 1
